{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b68b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, statistics, time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch.nn as nn\n",
    "import torch    \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Hugging Face authentication\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# Save token so it's used automatically in the background\n",
    "token = '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'\n",
    "HfFolder.save_token(token)\n",
    "\n",
    "# constants\n",
    "dataset = \"dank_memes\"\n",
    "pre_trained_model_checkpoint = \"roberta-base\"\n",
    "model_name = \"roberta-base-memes-900k-subset-75\"\n",
    "hub_model_id = \"armageddon/roberta-base-memes-900k-subset-75\"\n",
    "stride = 150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e5b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in meme dict dataset: dict_keys(['label_uuid_dic', 'uuid_label_dic', 'uuid_caption_dic', 'uuid_image_path_dic', 'uuid_caption_cased_dic'])\n",
      "Number of uuids: 300\n"
     ]
    }
   ],
   "source": [
    "meme_dict = None\n",
    "with open('../data/meme_900k_cleaned_data_v2.pkl', 'rb') as f:\n",
    "    meme_dict = pickle.load(f)\n",
    "print(\"Keys in meme dict dataset:\", meme_dict.keys())\n",
    "print(\"Number of uuids:\", len(meme_dict['uuid_label_dic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "682e8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def clean_and_unify_caption(caption):\n",
    "    return caption[0].strip()+', '+caption[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9368f4f4-8cae-44cf-9b88-cbef5bd4c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../data/training_label.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ebb3bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000 22500 22500\n"
     ]
    }
   ],
   "source": [
    "# create pandas dataframe\n",
    "temp_arr = []\n",
    "for uuid in labels.keys():\n",
    "    for caption in meme_dict['uuid_caption_dic'][uuid]:\n",
    "        temp_arr.append([uuid, clean_and_unify_caption(caption)])\n",
    "df = pd.DataFrame(temp_arr, columns=['category', 'text'])\n",
    "\n",
    "# split dataset\n",
    "np.random.seed(42)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14949b95-8128-42bf-b42c-16c755bc9995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6499b2508b2f4d1b8222fefccae74850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e2af74013944ab94c8f4dc25fd6f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73df2082df024d828e4157da0117b807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab284013a014df29296a3a0d78848ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876e01183ff941b9b0bf79dd9ec11e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3428e9f-b93c-4a2b-accb-f410aa61866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = [labels[label] for label in df['category']]\n",
    "        self.texts = [tokenizer(text, padding='max_length', max_length = 50, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6edb8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(df_train)\n",
    "val_dataset = Dataset(df_val)\n",
    "test_dataset = Dataset(df_test)\n",
    "# train_dataset = torch.load('../data/train_dataset')\n",
    "# val_dataset = torch.load('./models/data/val_dataset')\n",
    "# test_dataset = torch.load('./models/data/test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74eacce2-f2ba-4bbe-bc49-7eb98f6a6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoModel\n",
    "class Meme_Classifier(nn.Module):\n",
    "    def __init__(self, num_labels, dropout=0.3):\n",
    "        super(Meme_Classifier, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(pre_trained_model_checkpoint)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(768, 512)\n",
    "        self.linear2 = nn.Linear(512, num_labels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        _, pooled_output = self.model(input_ids=input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output1 = self.dropout(pooled_output)\n",
    "        linear_output1 = self.dropout(self.relu(self.linear1(dropout_output1)))\n",
    "        final_output = self.relu(self.linear2(linear_output1))\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c9fdb1a-17bc-4ed9-a536-2936be61ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(model, train_dataset, val_dataset, learning_rate, loss_diff, max_epochs):\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    epoch_num = 0\n",
    "    prev_loss = float('inf')\n",
    "    while True:\n",
    "            epoch_num+=1\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_dataset): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_dataset): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_dataset): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_dataset): .3f}')\n",
    "            \n",
    "            if epoch_num>=max_epochs or abs(prev_loss-total_loss_train)<=loss_diff:\n",
    "                break\n",
    "            prev_loss=total_loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9087c56-242d-42e0-9046-d304d0514a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70744cd65bb4d8eb8e459d8b25afd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 32%|███▏      | 1780/5625 [2:08:32<3:20:09,  3.12s/it]  "
     ]
    }
   ],
   "source": [
    "model = Meme_Classifier(len(labels))\n",
    "LR = 1e-6\n",
    "max_epochs = 20\n",
    "loss_diff = 0.01\n",
    "train(model, train_dataset, val_dataset, LR, loss_diff, max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e47dfe-a52c-45a8-9546-d3cc8d444cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '../models/roberta-base-memes-900k-subset-75'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208190c-0c7d-4527-b108-9ec5b00c5fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67543cc8-cd3d-4f25-a180-a12cf00802ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Meme_Classifier(len(labels))\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79d5a5-4fdb-4f7d-8f17-ab45845d33b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topKPrediction(ks, model, test_input, test_labels):\n",
    "    final_scores_dict = defaultdict(int)\n",
    "    mask = test_input['attention_mask'].to(device)\n",
    "    input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "    logits = model(input_id, mask).to('cpu')\n",
    "    argsorted_logits = torch.argsort(logits, dim=1, descending=True)\n",
    "    final_scores_dict = defaultdict(int)\n",
    "    for i in range(len(test_labels)):        \n",
    "        for k in ks:\n",
    "            if test_labels[i] in argsorted_logits[i][:k]:\n",
    "                final_scores_dict[k]+=1\n",
    "    return final_scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff1dc68-0042-46d8-9b3b-4a0683fc31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topKAccuracy(ks, model, test_dataset):\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64)\n",
    "    final_scores = defaultdict(int)\n",
    "    for test_input, test_labels in tqdm(test_dataloader):\n",
    "        scores_dict = topKPrediction(ks, model, test_input, test_labels)\n",
    "        for k in ks:\n",
    "            final_scores[k]+=scores_dict[k]\n",
    "    print(final_scores)\n",
    "    for k, v in final_scores.items():\n",
    "        final_scores[k] = v/len(test_dataset)\n",
    "    return final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84801b-22bc-433e-b90a-101898d08a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:11<00:00, 30.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {1: 249, 3: 1108, 5: 1767, 10: 3392})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {1: 0.011066666666666667,\n",
       "             3: 0.049244444444444445,\n",
       "             5: 0.07853333333333333,\n",
       "             10: 0.15075555555555556})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topKAccuracy([1,3,5,10], model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b86db23-ec90-4fcd-be34-ede25d54b3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:00<00:00, 10827.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# now test full user captions for accuracy\n",
    "import os\n",
    "import regex as re\n",
    "import pickle\n",
    "testing_user_captions = []\n",
    "dir_path = './memes900k_qa/'\n",
    "for path in tqdm(os.listdir(dir_path)):\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        if not re.match(r'.*_manual.pkl', path):\n",
    "            with open(os.path.join(dir_path, path), 'rb') as f:\n",
    "                dic = pickle.load(f)\n",
    "                for v in dic['qa'].keys(): \n",
    "                    testing_user_captions.append([v, labels[dic['uuid']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3842c0a-b11b-47a9-a3e4-a82f6be19702",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [tokenizer(text[0], padding='max_length', max_length = 50, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in testing_user_captions]\n",
    "# input_ids = torch.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208b46eb-d57d-4890-a873-55ffbb2d3df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  7424,   939,  3529,    65,    55, 15711,     9, 19982,  1666,\n",
       "             6,   117, 16506,    47, 14964,   101,   195,   416,   328,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]])}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac681f-d6e5-4fcb-87bb-e58371394884",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.stack([x['input_ids'] for x in tokenized])\n",
    "input_ids = input_ids.reshape(len(input_ids), -1).to(device)\n",
    "masks = torch.stack([x['attention_mask'] for x in tokenized])\n",
    "masks = masks.reshape(len(input_ids), -1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe7da1-40e5-4aad-83d2-f74ef1bd4fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3745/3745 [01:25<00:00, 43.71it/s]\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in tqdm(range(len(input_ids))):\n",
    "    logits = model(input_ids[i].reshape(1, -1), masks[i].reshape(1, -1))\n",
    "    acc += meme_accuracy_sum_only(logits, [testing_user_captions[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a969f-c668-482e-8799-b194d737e009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6085447263017356"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc/len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8f1d0-a9f3-401e-a88e-3d0c5b9c7d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
